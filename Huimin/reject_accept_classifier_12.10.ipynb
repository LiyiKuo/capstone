{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_raw_accepted = pd.read_csv('accepted_2007_to_2018Q4.csv.gz')\n",
    "df_raw_rejected = pd.read_csv('rejected_2007_to_2018Q4.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For applications prior to November 5, 2013 the risk score is the borrower's FICO score. \n",
    "For applications after November 5, 2013 the risk score is the borrower's Vantage score.\n",
    "'''\n",
    "\n",
    "def data_process(df_accepted,df_rejected):\n",
    "    \n",
    "    accepted = df_accepted.copy()\n",
    "    rejected = df_rejected.copy()\n",
    "    \n",
    "    # Processing accepted data \n",
    "    accepted = accepted[['loan_amnt', 'purpose', 'dti',\n",
    "       'zip_code', 'addr_state', 'emp_length', 'issue_d',\n",
    "       'loan_status', 'last_fico_range_high','last_fico_range_low']]\n",
    "    \n",
    "    accepted['issue_d'] = pd.to_datetime(accepted['issue_d'])\n",
    "    accepted['app_year'] = accepted['issue_d'].dt.year\n",
    "    accepted['app_month'] = accepted['issue_d'].dt.month\n",
    "\n",
    "    accepted['risk_score'] = (accepted['last_fico_range_high'] + accepted['last_fico_range_low'])/2\n",
    "    accepted = accepted.drop(columns = ['issue_d','last_fico_range_high','last_fico_range_low'])\n",
    "\n",
    "#     if reverse = False:\n",
    "#         accepted = accepted.replace({'loan_status' : { 'Charged Off' : 'rejected', 'Late (16-30 days)' : 'rejected', \n",
    "#                                        'Late (31-120 days)' : 'rejected',\n",
    "#                                        'Does not meet the credit policy. Status:Charged Off' : 'rejected',\n",
    "#                                        'Fully Paid' : 'accepted', 'Current' : 'accepted',\n",
    "#                                        'In Grace Period' : 'accepted',\n",
    "#                                        'Does not meet the credit policy. Status:Fully Paid' : 'accepted'}})\n",
    "\n",
    "    accepted['loan_status'] =1\n",
    "            \n",
    "    \n",
    "    # Processing rejected data \n",
    "    rejected = rejected.iloc[:,1:9]\n",
    "    rejected = rejected.rename(columns={\"Amount Requested\": \"loan_amnt\", \"Application Date\": \"app_date\", \"Loan Title\": \"purpose\",\n",
    "                             \"Risk_Score\": \"risk_score\", \"Debt-To-Income Ratio\": \"dti\", \"Zip Code\": \"zip_code\",\n",
    "                             \"State\": \"addr_state\", \"Employment Length\": \"emp_length\"})\n",
    "    rejected['loan_status'] = 0\n",
    "    \n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*consolidation.*$)', 'debt_consolidation')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*loan.*$)', 'debt_consolidation')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*debt.*$)', 'debt_consolidation')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*card.*$)', 'credit_card')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*credit.*$)', 'credit_card')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*house.*$)', 'house')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*business.*$)', 'small_business')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*car.*$)', 'car')\n",
    "    rejected['purpose'] = rejected['purpose'].str.replace(r'(^.*education.*$)', 'education')\n",
    "    \n",
    "    \n",
    "    rejected['purpose'] = np.where(rejected['purpose'].isin(['house', 'credit_card', 'major_purchase', 'debt_consolidation',\n",
    "       'other', 'moving', 'small_business', 'home_improvement',\n",
    "       'vacation', 'medical', 'renewable_energy', 'car', 'wedding']), rejected['purpose'], 'other')\n",
    "\n",
    "    rejected['app_date'] = pd.to_datetime(rejected['app_date'])\n",
    "    rejected['app_year'] = rejected['app_date'].dt.year\n",
    "    rejected['app_month'] = rejected['app_date'].dt.month\n",
    "\n",
    "    rejected = rejected.drop(columns = 'app_date')\n",
    "    \n",
    "    rejected['dti'] = rejected['dti'].str.replace('%','')\n",
    "    \n",
    "    # Merge accepted and rejected data for later process\n",
    "    df_processed = pd.concat([accepted, rejected], sort=True)\n",
    "    \n",
    "    #Drop zip Code\n",
    "    df_processed=df_processed.drop(['zip_code'],axis=1)\n",
    "    \n",
    "    # Convert categorical to numerical-- 10 means more than 10 years \n",
    "    df_processed['emp_length'] = df_processed['emp_length'].str.extract('(\\d+)')\n",
    "\n",
    "\n",
    "    # Missing Values: risk_score, emp_length, dti\n",
    "    df_processed['emp_length'] = df_processed['emp_length'].fillna(0)\n",
    "    df_processed['emp_length'] = df_processed['emp_length'].astype(int)\n",
    "    df_processed['risk_score'] = df_processed['risk_score'].fillna(0)\n",
    "    df_processed['dti'] = df_processed['dti'].fillna(0)\n",
    "    df_processed['dti'] = df_processed['dti'].astype(float)\n",
    "    \n",
    "    df_processed = df_processed.drop(['purpose'], axis = 1)\n",
    "    df_processed = pd.get_dummies(df_processed)\n",
    "    \n",
    "    \n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2d461afdd0c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#downsample data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_raw_accepted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_raw_rejected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-562cd5a15a49>\u001b[0m in \u001b[0;36mdata_process\u001b[1;34m(df_accepted, df_rejected)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mdf_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'purpose'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mdf_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_processed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    857\u001b[0m             dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\n\u001b[0;32m    858\u001b[0m                                     \u001b[0mdummy_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_na\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                                     drop_first=drop_first, dtype=dtype)\n\u001b[0m\u001b[0;32m    860\u001b[0m             \u001b[0mwith_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m         \u001b[0mdummy_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Data Process the Raw Data\n",
    "df_processed = data_process(df_raw_accepted,df_raw_rejected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling imbalanced data by downsize the majority from the original dataset\n",
    "\n",
    "# The predicted column is \"loan_status\" , split the data into training and test sets.\n",
    "#test set will  be used for final model accuracy test\n",
    "#train set will be used for testing models\n",
    "\n",
    "x_m = df_processed.drop(['loan_status'], axis=1)\n",
    "y_m = df_processed.loc[:,'loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_m, y_m, test_size=0.2, random_state=42)\n",
    "\n",
    "#downsampling train set\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "reject=X[X.loan_status==0] #reject\n",
    "accept=X[X.loan_status==1]  #accept\n",
    "\n",
    "reject_downsampled = resample(reject,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(accept), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([reject_downsampled, accept])\n",
    "y_train_balanced = downsampled.loan_status\n",
    "X_train_balanced = downsampled.drop('loan_status', axis=1)\n",
    "\n",
    "y_train_d.to_csv('balanced_y_train.csv')\n",
    "X_train_d.to_csv('balanced_x_train.csv')\n",
    "X_test.to_csv('unbalanced_X_test')\n",
    "y_test.to_csv('unbalanced_y_test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsample test set\n",
    "\n",
    "# concatenate our test data back together\n",
    "X_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "reject_test=X_test[X_test.loan_status==0] #reject\n",
    "accept_test=X_test[X_test.loan_status==1]  #accept\n",
    "\n",
    "reject_downsampled_test = resample(reject_test,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(accept_test), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled_test = pd.concat([reject_downsampled_test, accept_test])\n",
    "y_test_balanced = downsampled.loan_status\n",
    "X_test_balanced = downsampled.drop('loan_status', axis=1)\n",
    "y_test_balanced.to_csv('balanced_y_test')\n",
    "X_test_balanced.to_csv('balanced_X_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
